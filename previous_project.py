# -*- coding: utf-8 -*-
"""CS445 Group #1 - Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tl_bcY-Jlzr2kRoD9E36si9v1lDK6A4N

# Group #1 - Project

Members:
- Serhan YILMAZ - 31275
- Sadiq Qara - 30410
- Furkan Eris - insert number

## Section #1 - Benchmark Creation with Naive Bayes and Logistic Regression
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix
import re

def load_data(file_path):
    """
    Load data file with proper handling of delimiters and encoding
    """
    try:
        # First attempt - try comma delimiter with latin1 encoding
        df = pd.read_csv(
            file_path,
            encoding='latin1',
            engine='python',
            on_bad_lines='skip'
        )

        # Check if we got one column with everything
        if len(df.columns) == 1:
            print("File appears to be tab-delimited but was read as CSV. Trying again with tabs...")

            # Try again with tab delimiter
            df = pd.read_csv(
                file_path,
                sep='\t',
                encoding='latin1',
                engine='python',
                on_bad_lines='skip'
            )

        print(f"Successfully loaded {len(df)} rows from {file_path}")

        # Clean up column names (remove any extra quotes or spaces)
        df.columns = [col.strip().strip('"').strip("'") for col in df.columns]

        # Verify expected columns
        expected_columns = ['Tweet', 'Target', 'Stance', 'Opinion Towards', 'Sentiment']
        missing_columns = [col for col in expected_columns if col not in df.columns]

        if missing_columns:
            raise ValueError(f"Missing expected columns: {missing_columns}")

        return df

    except Exception as e:
        print(f"Error loading {file_path}: {str(e)}")

        # Try one more time with direct file reading
        try:
            print("\nAttempting to read file directly...")
            with open(file_path, 'r', encoding='latin1') as file:
                lines = file.readlines()

            # Process headers
            headers = lines[0].strip().split('\t')
            headers = [h.strip().strip('"').strip("'") for h in headers]

            # Process data
            data = []
            for line in lines[1:]:
                values = line.strip().split('\t')
                if len(values) == len(headers):
                    row = dict(zip(headers, values))
                    data.append(row)

            df = pd.DataFrame(data)
            print(f"Successfully loaded {len(df)} rows using direct file reading")
            return df

        except Exception as e2:
            print(f"Direct file reading failed: {str(e2)}")
            raise

class StanceDetector:
    def __init__(self):
        self.nb_pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                lowercase=True,
                max_features=5000,
                stop_words='english'
            )),
            ('classifier', MultinomialNB())
        ])

        self.lr_pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                lowercase=True,
                max_features=5000,
                stop_words='english'
            )),
            ('classifier', LogisticRegression(
                max_iter=1000,
                multi_class='multinomial',
                class_weight='balanced'
            ))
        ])

    def preprocess_text(self, text):
        # Convert to lowercase
        text = str(text).lower()
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        # Remove user mentions
        text = re.sub(r'@\w+', '', text)
        # Remove hashtags symbol (but keep the text)
        text = re.sub(r'#', '', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def prepare_data(self, df):
        # Combine Tweet and Target columns for context
        df['processed_text'] = df['Tweet'].apply(self.preprocess_text) + ' ' + df['Target']
        return df['processed_text'], df['Stance']

    def train(self, train_df):
        # Prepare training data
        X_train, y_train = self.prepare_data(train_df)

        print("\nClass distribution in training data:")
        print(y_train.value_counts())

        # Train both models
        print("\nTraining Naive Bayes model...")
        self.nb_pipeline.fit(X_train, y_train)

        print("Training Logistic Regression model...")
        self.lr_pipeline.fit(X_train, y_train)

    def evaluate(self, test_df):
        # Prepare test data
        X_test, y_test = self.prepare_data(test_df)

        # Evaluate Naive Bayes
        nb_predictions = self.nb_pipeline.predict(X_test)
        print("\nNaive Bayes Results:")
        print("\nClassification Report:")
        print(classification_report(y_test, nb_predictions))
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_test, nb_predictions))

        # Evaluate Logistic Regression
        lr_predictions = self.lr_pipeline.predict(X_test)
        print("\nLogistic Regression Results:")
        print("\nClassification Report:")
        print(classification_report(y_test, lr_predictions))
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_test, lr_predictions))

try:
    # Load and preprocess training data
    print("Loading training data...")
    train_df = load_data('train.csv')

    print("\nTraining data preview:")
    print("\nColumns:", train_df.columns.tolist())
    print("\nShape:", train_df.shape)
    print("\nFirst few rows:")
    print(train_df.head(2))

    # Initialize and train model
    detector = StanceDetector()
    detector.train(train_df)

    # Load and evaluate test data
    print("\nLoading test data...")
    test_df = load_data('test.csv')
    detector.evaluate(test_df)

except Exception as e:
    print(f"\nAn error occurred: {str(e)}")
    print("\nPlease check your data files and provide the following information:")
    print("1. How was the file created/exported?")
    print("2. Can you open it in a text editor and verify the delimiter?")
    print("3. Are there any special characters in the text?")

"""## 2. Zero Shot"""

from transformers import AutoTokenizer, AutoModel
import torch

# Load BERT tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
bert_model = AutoModel.from_pretrained('bert-base-uncased')

# Move model to the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
bert_model.to(device)
bert_model.eval()

# Modify process_data to generate embeddings
def process_data(df):
    """
    Process data from a DataFrame and generate embeddings using BERT.
    """
    texts = df['post'].tolist()  # Raw post text
    topics = df['new_topic'].tolist()  # Topics
    labels = df['label'].tolist()  # Labels (stance)

    text_embeddings = []
    topic_embeddings = []

    for text, topic in zip(texts, topics):
        # Tokenize and encode text
        text_inputs = tokenizer(
            text, return_tensors='pt', truncation=True, padding=True, max_length=128
        ).to(device)
        # Tokenize and encode topic
        topic_inputs = tokenizer(
            topic, return_tensors='pt', truncation=True, padding=True, max_length=32
        ).to(device)

        # Generate embeddings
        with torch.no_grad():
            text_emb = bert_model(**text_inputs).last_hidden_state[:, 0, :]  # CLS token
            topic_emb = bert_model(**topic_inputs).last_hidden_state[:, 0, :]  # CLS token

        text_embeddings.append(text_emb.squeeze(0))
        topic_embeddings.append(topic_emb.squeeze(0))

    return text_embeddings, topic_embeddings, labels


# Define the dataset class with padding
class PaddedStanceDataset(Dataset):
    def __init__(self, text_embeddings, topic_embeddings, labels):
        self.text_embeddings = text_embeddings  # List of text embeddings
        self.topic_embeddings = topic_embeddings  # List of topic embeddings
        self.labels = labels

    def __len__(self):
        return len(self.text_embeddings)

    def __getitem__(self, idx):
        return {
            'text': self.text_embeddings[idx],  # Already in tensor form
            'topic': self.topic_embeddings[idx],  # Already in tensor form
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }


# Define a custom collate function for padding
def collate_fn(batch):
    texts = [item['text'] for item in batch]
    topics = [item['topic'] for item in batch]
    labels = [item['labels'] for item in batch]

    # Pad the sequences
    texts_padded = pad_sequence(texts, batch_first=True)
    topics_padded = pad_sequence(topics, batch_first=True)

    return {
        'text': texts_padded,
        'topic': topics_padded,
        'labels': torch.tensor(labels, dtype=torch.long)
    }


# Process the data
train_tokenized_texts, train_tokenized_topics, train_labels = process_data(train_df)
dev_tokenized_texts, dev_tokenized_topics, dev_labels = process_data(dev_df)
test_tokenized_texts, test_tokenized_topics, test_labels = process_data(test_df)

# Create datasets
train_dataset = PaddedStanceDataset(train_tokenized_texts, train_tokenized_topics, train_labels)
dev_dataset = PaddedStanceDataset(dev_tokenized_texts, dev_tokenized_topics, dev_labels)
test_dataset = PaddedStanceDataset(test_tokenized_texts, test_tokenized_topics, test_labels)

# Create DataLoaders with the custom collate function
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)

# Verify DataLoader outputs
for batch in train_loader:
    text, topic, labels = batch['text'], batch['topic'], batch['labels']
    print(f"text shape: {text.shape}, topic shape: {topic.shape}, labels shape: {labels.shape}")
    break

class FFNN(nn.Module):
    def __init__(self, use_cuda, input_dim, hidden_size, in_dropout_prob, add_topic=False):
        super(FFNN, self).__init__()
        self.add_topic = add_topic
        self.use_cuda = use_cuda

        # If topic input is added, double the input dimension
        total_input_dim = input_dim * 2 if add_topic else input_dim

        # Define layers
        self.fc1 = nn.Linear(total_input_dim, hidden_size)
        self.dropout = nn.Dropout(in_dropout_prob)
        self.fc2 = nn.Linear(hidden_size, 3)  # Assuming 3 output classes

    def forward(self, text_rep, topic_rep=None):
        """
        Forward pass for FFNN.
        :param text_rep: Representation of the text input.
        :param topic_rep: Optional representation of the topic input.
        :return: Logits for classification.
        """
        if self.add_topic and topic_rep is not None:
            combined_rep = torch.cat([text_rep, topic_rep], dim=-1)
        else:
            combined_rep = text_rep

        x = self.fc1(combined_rep)
        x = torch.relu(x)
        x = self.dropout(x)
        logits = self.fc2(x)
        return logits


class ScaledDotProductAttention(nn.Module):
    def __init__(self, input_dim):
        super(ScaledDotProductAttention, self).__init__()
        self.scale = 1 / (input_dim ** 0.5)  # Scaling factor

    def forward(self, query, key, value=None):
        """
        Compute scaled dot-product attention.
        :param query: Query tensor of shape (batch_size, seq_len, input_dim)
        :param key: Key tensor of shape (batch_size, seq_len, input_dim)
        :param value: Value tensor (optional), same shape as key if provided
        :return: Attention-weighted output tensor
        """
        scores = torch.bmm(query, key.transpose(1, 2))  # Compute similarity scores
        scores = scores * self.scale  # Scale scores
        attn_weights = torch.softmax(scores, dim=-1)  # Normalize to get attention weights

        if value is not None:
            output = torch.bmm(attn_weights, value)  # Weighted sum
        else:
            output = torch.bmm(attn_weights, query)  # Self-attention

        return output


class TGANet(nn.Module):
    ## Topic-grouped Attention Network
    def __init__(self, **kwargs):
        super(TGANet, self).__init__()
        self.use_cuda = kwargs['use_cuda']
        self.hidden_dim = kwargs['hidden_size']
        self.input_dim = kwargs['text_dim']
        self.num_labels = 3

        self.attention_mode = kwargs.get('att_mode', 'text_only')
        self.learned = kwargs['learned']

        # Scaled Dot-Product Attention
        self.att = ScaledDotProductAttention(self.input_dim)

        # Topic transformation layer
        self.topic_trans = nn.Linear(kwargs['topic_dim'], self.input_dim)

        # Feed-Forward Neural Network
        self.ffnn = FFNN(
            use_cuda=self.use_cuda,
            input_dim=self.input_dim,
            hidden_size=self.hidden_dim,
            in_dropout_prob=kwargs['in_dropout_prob'],
            add_topic=True,
        )

    def forward(self, text, topic):
        """
        Forward pass for TGANet.
        :param text: Input text tensor (batch_size, seq_len, input_dim)
        :param topic: Input topic tensor (batch_size, seq_len, topic_dim)
        :return: Predicted logits
        """
        # Average text embeddings over the sequence length
        avg_text = text.mean(dim=1)  # Shape: (batch_size, input_dim)

        # Apply topic transformation
        topic_rep = self.topic_trans(topic)  # Shape: (batch_size, seq_len, input_dim)

        # Attention mechanism
        gen_rep = self.att(topic_rep, topic_rep)  # Shape: (batch_size, seq_len, input_dim)
        gen_rep = gen_rep.mean(dim=1)  # Reduce seq_len dimension: (batch_size, input_dim)

        # Pass through FFNN
        preds = self.ffnn(avg_text, gen_rep)  # Shape: (batch_size, num_labels)

        return preds

# Initialize model
tganet = TGANet(
    use_cuda=use_cuda,
    hidden_size=128,
    text_dim=768,
    topic_dim=768,
    in_dropout_prob=0.1,
    att_mode='text_only',
    learned=True
)

if use_cuda:
    tganet.cuda()

# Step 2: Define optimizer and loss function
optimizer = optim.Adam(tganet.parameters(), lr=0.001)  # Adjust learning rate if needed
loss_fn = nn.CrossEntropyLoss()

# Step 3: Training loop
def train_tganet(model, dataloader, optimizer, loss_fn, num_epochs, dev_dataloader=None, early_stopping=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    best_dev_loss = float('inf')

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for batch in dataloader:
            text, topic, labels = batch['text'].to(device), batch['topic'].to(device), batch['labels'].to(device)

            optimizer.zero_grad()
            preds = model(text, topic)  # Simplified forward call
            loss = loss_fn(preds, labels)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}")

        # Dev evaluation
        if dev_dataloader:
            model.eval()
            dev_loss = 0
            with torch.no_grad():
                for dev_batch in dev_dataloader:
                    text, topic, labels = (
                        dev_batch['text'].to(device),
                        dev_batch['topic'].to(device),
                        dev_batch['labels'].to(device),
                    )

                    preds = model(text, topic)
                    loss = loss_fn(preds, labels)
                    dev_loss += loss.item()

            dev_loss /= len(dev_dataloader)
            print(f"Dev Loss: {dev_loss}")

            # Early stopping
            if early_stopping and dev_loss >= best_dev_loss:
                print("Early stopping triggered.")
                break
            best_dev_loss = dev_loss

    print("Training complete.")

'''
for batch in train_loader:
    text, topic, labels = batch['text'], batch['topic'], batch['labels']
    print("Starting forward pass...")
    preds = tganet(text, topic)
    print(f"Predictions shape: {preds.shape}")
    break
'''
# Step 4: Train the model
num_epochs = 10  # Adjust as needed
train_tganet(tganet, train_loader, optimizer, loss_fn, num_epochs, dev_dataloader=dev_loader)

"""## Section 2 - ROBERTA Stance Detection Model"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np
from tqdm import tqdm

class StanceDataset(Dataset):
    def __init__(self, texts, targets, labels=None, tokenizer=None, max_length=128):
        self.texts = texts
        self.targets = targets
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        target = str(self.targets[idx])

        # Combine text and target with special separator
        combined_text = f"{text} [SEP] {target}"

        encoding = self.tokenizer.encode_plus(
            combined_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

class StanceClassifier(nn.Module):
    def __init__(self, n_classes=3, dropout=0.1):
        super(StanceClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')
        self.drop = nn.Dropout(p=dropout)

        # Target-specific attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(768, 768),
            nn.Tanh(),
            nn.Linear(768, 1)
        )

        # Classification layers
        self.pre_classifier = nn.Linear(768, 768)
        self.classifier = nn.Linear(768, n_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )

        sequence_output = outputs.last_hidden_state

        # Apply attention
        attention_weights = self.attention(sequence_output).squeeze(-1)
        attention_weights = attention_weights * attention_mask  # Mask padded positions
        attention_weights = torch.softmax(attention_weights, dim=1)

        # Weighted sum of token representations
        weighted_output = torch.bmm(
            attention_weights.unsqueeze(1),
            sequence_output
        ).squeeze(1)

        # Classification
        pooled_output = self.drop(weighted_output)
        pooled_output = self.pre_classifier(pooled_output)
        pooled_output = nn.ReLU()(pooled_output)
        pooled_output = self.drop(pooled_output)
        logits = self.classifier(pooled_output)

        return logits

class StanceDetectionSystem:
    def __init__(self, n_classes=3, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
        self.model = StanceClassifier(n_classes=n_classes)
        self.device = device
        self.model.to(self.device)

    def prepare_data(self, df, batch_size=16):
        dataset = StanceDataset(
            texts=df['Tweet'].values,
            targets=df['Target'].values,
            labels=df['Stance'].values if 'Stance' in df.columns else None,
            tokenizer=self.tokenizer
        )

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True if 'Stance' in df.columns else False
        )

    def train(self, train_df, eval_df, epochs=3, batch_size=16, learning_rate=2e-5):
        train_dataloader = self.prepare_data(train_df, batch_size)
        eval_dataloader = self.prepare_data(eval_df, batch_size)

        optimizer = AdamW(self.model.parameters(), lr=learning_rate)
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=total_steps
        )

        best_eval_f1 = 0
        for epoch in range(epochs):
            print(f'\nEpoch {epoch + 1}/{epochs}')

            # Training
            self.model.train()
            total_train_loss = 0
            for batch in tqdm(train_dataloader, desc='Training'):
                self.model.zero_grad()

                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask)
                loss = nn.CrossEntropyLoss()(outputs, labels)

                total_train_loss += loss.item()
                loss.backward()

                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                scheduler.step()

            avg_train_loss = total_train_loss / len(train_dataloader)
            print(f'Average training loss: {avg_train_loss}')

            # Evaluation
            eval_results = self.evaluate(eval_df, batch_size)
            print('\nEvaluation Results:')
            print(eval_results['classification_report'])

            # Save best model
            if eval_results['macro_f1'] > best_eval_f1:
                best_eval_f1 = eval_results['macro_f1']
                torch.save(self.model.state_dict(), 'best_stance_model.pt')

    def evaluate(self, eval_df, batch_size=16):
        eval_dataloader = self.prepare_data(eval_df, batch_size)

        self.model.eval()
        predictions = []
        actual_labels = []

        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                outputs = self.model(input_ids, attention_mask)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().numpy())
                actual_labels.extend(batch['labels'].cpu().numpy())

        # Calculate metrics
        classification_rep = classification_report(actual_labels, predictions)
        conf_matrix = confusion_matrix(actual_labels, predictions)

        # Calculate macro F1 score for model selection
        clf_dict = classification_report(actual_labels, predictions, output_dict=True)
        macro_f1 = clf_dict['macro avg']['f1-score']

        return {
            'predictions': predictions,
            'classification_report': classification_rep,
            'confusion_matrix': conf_matrix,
            'macro_f1': macro_f1
        }

def prepare_stance_labels(df):
    """Convert stance labels to numeric values"""
    stance_mapping = {'AGAINST': 0, 'NONE': 1, 'FAVOR': 2}
    df['Stance'] = df['Stance'].map(stance_mapping)
    return df

# Load data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

# Prepare labels
train_df = prepare_stance_labels(train_df)
test_df = prepare_stance_labels(test_df)

# Initialize system
system = StanceDetectionSystem()

# Train and evaluate
print("Training model...")
system.train(
    train_df=train_df,
    eval_df=test_df,
    epochs=10,
    batch_size=64,
    learning_rate=1e-4
)

# Final evaluation
print("\nFinal Evaluation on Test Set:")
final_results = system.evaluate(test_df)
print(final_results['classification_report'])
print("\nConfusion Matrix:")
print(final_results['confusion_matrix'])

"""## Step 3 - DeBERTa-v3 Stance Detection Model"""

!pip install emoji

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup
from sklearn.metrics import classification_report, confusion_matrix
import pandas as pd
import numpy as np
from tqdm import tqdm
import re
import emoji
from collections import Counter
import math

class StanceDataset(Dataset):
    def __init__(self, texts, targets, labels=None, tokenizer=None, max_length=256):
        self.texts = texts
        self.targets = targets
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def clean_text(self, text):
        # Convert to string and lowercase
        text = str(text).lower()
        # Replace URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '[URL]', text, flags=re.MULTILINE)
        # Convert emojis to text
        text = emoji.demojize(text)
        # Remove user mentions
        text = re.sub(r'@\w+', '@USER', text)
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.clean_text(self.texts[idx])
        target = self.clean_text(self.targets[idx])

        # Enhanced input format with explicit stance markers
        combined_text = f"tweet: {text} [SEP] target: {target} [SEP] stance classification"

        encoding = self.tokenizer.encode_plus(
            combined_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors='pt'
        )

        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'token_type_ids': encoding['token_type_ids'].flatten()
        }

        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None):
        query_layer = self.transpose_for_scores(self.query(hidden_states))
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))

        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = nn.Softmax(dim=-1)(attention_scores)
        attention_probs = self.dropout(attention_probs)

        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        attention_output = self.dense(context_layer)
        attention_output = self.dropout(attention_output)
        attention_output = self.LayerNorm(attention_output + hidden_states)

        return attention_output

class StanceClassifier(nn.Module):
    def __init__(self, n_classes=3):
        super(StanceClassifier, self).__init__()
        self.model_name = 'microsoft/deberta-v3-large'
        self.model = AutoModel.from_pretrained(self.model_name)
        config = self.model.config

        self.stance_attention = MultiHeadAttention(config)

        # Multiple classification layers
        self.pre_classifier = nn.Sequential(
            nn.Linear(config.hidden_size, config.hidden_size),
            nn.LayerNorm(config.hidden_size),
            nn.Dropout(0.2),
            nn.ReLU(),
            nn.Linear(config.hidden_size, config.hidden_size // 2)
        )

        self.classifier = nn.Sequential(
            nn.LayerNorm(config.hidden_size // 2),
            nn.Dropout(0.2),
            nn.ReLU(),
            nn.Linear(config.hidden_size // 2, n_classes)
        )

    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True
        )

        sequence_output = outputs.last_hidden_state

        # Apply stance-specific attention
        attended_output = self.stance_attention(sequence_output, attention_mask.unsqueeze(1).unsqueeze(2))

        # Pool the output (use [CLS] token representation)
        pooled_output = attended_output[:, 0, :]

        # Classification
        pre_logits = self.pre_classifier(pooled_output)
        logits = self.classifier(pre_logits)

        return logits

class StanceDetectionSystem:
    def __init__(self, n_classes=3, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')
        self.model = StanceClassifier(n_classes=n_classes)
        self.device = device
        self.model.to(self.device)

    def prepare_data(self, df, batch_size=8):
        dataset = StanceDataset(
            texts=df['Tweet'].values,
            targets=df['Target'].values,
            labels=df['Stance'].values if 'Stance' in df.columns else None,
            tokenizer=self.tokenizer
        )

        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True if 'Stance' in df.columns else False
        )

    def compute_class_weights(self, labels):
        class_counts = Counter(labels)
        total = sum(class_counts.values())
        class_weights = {cls: total / count for cls, count in class_counts.items()}
        return torch.FloatTensor([class_weights[i] for i in range(len(class_weights))]).to(self.device)

    def train(self, train_df, eval_df, epochs=5, batch_size=8, learning_rate=1e-5):
        train_dataloader = self.prepare_data(train_df, batch_size)
        eval_dataloader = self.prepare_data(eval_df, batch_size)

        # Calculate class weights for balanced loss
        class_weights = self.compute_class_weights(train_df['Stance'].values)

        # Optimizer with weight decay
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.model.named_parameters()
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': 0.01
            },
            {
                'params': [p for n, p in self.model.named_parameters()
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]

        optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)

        # Learning rate scheduler
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=total_steps // 10,
            num_training_steps=total_steps
        )

        best_eval_f1 = 0
        for epoch in range(epochs):
            print(f'\nEpoch {epoch + 1}/{epochs}')

            # Training
            self.model.train()
            total_train_loss = 0

            for batch in tqdm(train_dataloader, desc='Training'):
                self.model.zero_grad()

                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                token_type_ids = batch['token_type_ids'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, token_type_ids)

                # Weighted CrossEntropy Loss
                loss = nn.CrossEntropyLoss(weight=class_weights)(outputs, labels)

                total_train_loss += loss.item()
                loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

                optimizer.step()
                scheduler.step()

            avg_train_loss = total_train_loss / len(train_dataloader)
            print(f'Average training loss: {avg_train_loss}')

            # Evaluation
            eval_results = self.evaluate(eval_df, batch_size)
            print('\nEvaluation Results:')
            print(eval_results['classification_report'])

            # Save best model
            if eval_results['macro_f1'] > best_eval_f1:
                best_eval_f1 = eval_results['macro_f1']
                torch.save(self.model.state_dict(), 'best_stance_model.pt')
                print(f'New best model saved with macro F1: {best_eval_f1:.4f}')

    def evaluate(self, eval_df, batch_size=8):
        eval_dataloader = self.prepare_data(eval_df, batch_size)

        self.model.eval()
        predictions = []
        actual_labels = []

        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                token_type_ids = batch['token_type_ids'].to(self.device)

                outputs = self.model(input_ids, attention_mask, token_type_ids)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().numpy())
                actual_labels.extend(batch['labels'].cpu().numpy())

        classification_rep = classification_report(actual_labels, predictions)
        conf_matrix = confusion_matrix(actual_labels, predictions)

        clf_dict = classification_report(actual_labels, predictions, output_dict=True)
        macro_f1 = clf_dict['macro avg']['f1-score']

        return {
            'predictions': predictions,
            'classification_report': classification_rep,
            'confusion_matrix': conf_matrix,
            'macro_f1': macro_f1
        }

def prepare_stance_labels(df):
    """Convert stance labels to numeric values"""
    stance_mapping = {'AGAINST': 0, 'NONE': 1, 'FAVOR': 2}
    df['Stance'] = df['Stance'].map(stance_mapping)
    return df

# Load and prepare data
train_df = load_data('train.csv')
test_df = load_data('test.csv')

train_df = prepare_stance_labels(train_df)
test_df = prepare_stance_labels(test_df)

# Initialize system
system = StanceDetectionSystem()

# Train and evaluate
print("Training model...")
system.train(
    train_df=train_df,
    eval_df=test_df,
    epochs=5,
    batch_size=8,
    learning_rate=1e-5
)

# Final evaluation
print("\nFinal Evaluation on Test Set:")
final_results = system.evaluate(test_df)
print(final_results['classification_report'])
print("\nConfusion Matrix:")
print(final_results['confusion_matrix'])